{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, chi2, f_classif\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer, MissingIndicator\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(r'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splitting title, first and last names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'Title'] = df_train.loc[:,'Name'].apply(lambda x: re.search(r\"(.+?)\\s(.+?)\\s(.+)\", x).group(1))\n",
    "df_train.loc[:, 'First_Name'] = df_train.loc[:,'Name'].apply(lambda x: re.search(r\"(.+?)\\s(.+?)\\s(.+)\", x).group(2))\n",
    "df_train.loc[:, 'Last_Name'] = df_train.loc[:,'Name'].apply(lambda x: re.search(r\"(.+?)\\s(.+?)\\s(.+)\", x).group(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[:, 'Title'] = df_test.loc[:,'Name'].apply(lambda x: re.search(r\"(.+?)\\s(.+?)\\s(.+)\", x).group(1))\n",
    "df_test.loc[:, 'First_Name'] = df_test.loc[:,'Name'].apply(lambda x: re.search(r\"(.+?)\\s(.+?)\\s(.+)\", x).group(2))\n",
    "df_test.loc[:, 'Last_Name'] = df_test.loc[:,'Name'].apply(lambda x: re.search(r\"(.+?)\\s(.+?)\\s(.+)\", x).group(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dictionary ={'Miss' : 'F', 'Ms.' : 'F', 'Mr.' : 'M', 'Master' : 'M'} \n",
    "df_train.loc[:,'gender'] = df_train.loc[:,'Title'].map(gender_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dictionary ={'Miss' : 'F', 'Mrs.' : 'F', 'Mr.' : 'M', 'Master' : 'M'} \n",
    "df_test.loc[:,'gender'] = df_test.loc[:,'Title'].map(gender_dictionary) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filling missing values:\n",
    "\n",
    "__Medical_tent__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Medical_Tent = df_train.Medical_Tent.fillna('No_tent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.Medical_Tent = df_test.Medical_Tent.fillna('No_tent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Birthday_year__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(df_train.loc[:,['Birthday_year']])\n",
    "df_train.loc[:,'Birthday_year'] = imputer.transform(df_train.loc[:,['Birthday_year']]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[:,'Birthday_year'] = imputer.transform(df_test.loc[:,['Birthday_year']]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__City__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:,'City'] = df_train.loc[:,'City'].fillna('Santa Fe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['Medical_Tent', 'City', 'gender']:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_train.loc[:, feature])\n",
    "    df_train.loc[:, feature] = encoder.transform(df_train.loc[:, feature])\n",
    "    df_test.loc[:, feature] = encoder.transform(df_test.loc[:, feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating expenses per capita from medical_expenses_family:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:,'Expenses_per_capita'] = df_train.loc[:,'Medical_Expenses_Family']/(df_train.loc[:,'Parents or siblings infected']+df_train.loc[:,'Wife/Husband or children infected']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[:,'Expenses_per_capita'] = df_test.loc[:,'Medical_Expenses_Family']/(df_test.loc[:,'Parents or siblings infected']+df_test.loc[:,'Wife/Husband or children infected']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropping some variables to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['Patient_ID','Family_Case_ID','Name','Title','First_Name','Last_Name','Deceased',\n",
    "                           'Medical_Expenses_Family'])\n",
    "y = df_train['Deceased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = df_test.drop(columns=['Patient_ID','Family_Case_ID','Name','Title','First_Name','Last_Name',\n",
    "                                'Medical_Expenses_Family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            'scaler', MinMaxScaler()\n",
    "        ),\n",
    "        (\n",
    "            'classifier', BaggingClassifier(\n",
    "                base_estimator= XGBClassifier(\n",
    "                    colsample_bytree = 1.0, \n",
    "                    eta = 0.63, \n",
    "                    max_depth = 2\n",
    "                ),\n",
    "                n_estimators=10,\n",
    "                max_features=1.0,\n",
    "                bootstrap=True,\n",
    "                random_state=42\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'scaler': [\n",
    "        MinMaxScaler(), \n",
    "        MinMaxScaler(feature_range=(-1,1)),\n",
    "        RobustScaler(),\n",
    "        StandardScaler(),\n",
    "        PowerTransformer()\n",
    "    ],\n",
    "    'classifier__n_estimators': list(np.arange(5,50,1)),\n",
    "    'classifier__max_features': list(np.arange(2,9,1)),\n",
    "    'classifier__bootstrap': [True, False],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(\n",
    "    pipe, \n",
    "    parameter_space, \n",
    "    cv=5, \n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    "    scoring='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3150 candidates, totalling 15750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 22.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 27.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed: 33.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed: 40.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed: 45.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10352 tasks      | elapsed: 50.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11689 tasks      | elapsed: 56.3min\n",
      "[Parallel(n_jobs=-1)]: Done 13106 tasks      | elapsed: 63.6min\n",
      "[Parallel(n_jobs=-1)]: Done 14605 tasks      | elapsed: 73.0min\n",
      "[Parallel(n_jobs=-1)]: Done 15750 out of 15750 | elapsed: 79.8min finished\n"
     ]
    }
   ],
   "source": [
    "best_model = gridsearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_features': 7,\n",
       " 'classifier__n_estimators': 9,\n",
       " 'scaler': MinMaxScaler(copy=True, feature_range=(0, 1))}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.8400000000000001\n",
      "Split 1:  0.8666666666666667\n",
      "Split 2:  0.8333333333333334\n",
      "Split 3:  0.8444444444444444\n",
      "Split 4:  0.8333333333333334\n",
      "Split 5:  0.8222222222222222\n"
     ]
    }
   ],
   "source": [
    "print('Mean: ', best_model.cv_results_['mean_test_score'][best_model.best_index_])\n",
    "print('Split 1: ', best_model.cv_results_['split0_test_score'][best_model.best_index_])\n",
    "print('Split 2: ', best_model.cv_results_['split1_test_score'][best_model.best_index_])\n",
    "print('Split 3: ', best_model.cv_results_['split2_test_score'][best_model.best_index_])\n",
    "print('Split 4: ', best_model.cv_results_['split3_test_score'][best_model.best_index_])\n",
    "print('Split 5: ', best_model.cv_results_['split4_test_score'][best_model.best_index_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final_model_3 = best_model.predict(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "version3 = pd.DataFrame({'Patient_ID': list(df_test.Patient_ID.values), 'Deceased':list(pred_final_model_3)}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "version3.to_csv('m20190922_version3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
